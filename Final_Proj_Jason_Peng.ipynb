{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d2b7250-ed9e-4dad-ab40-d126c1215bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from scipy.special import expit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import (accuracy_score,confusion_matrix, f1_score, roc_auc_score, roc_curve, auc)\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "URL_PATTERN = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "HTML_PATTERN = re.compile(r\"<.*?>\")\n",
    "ALLOWED_CHARS = re.compile(r\"[^a-z0-9\\.\\!\\?\\'\\s]\")\n",
    "CONTRACTIONS = {\"can't\":\"cannot\",\"won't\":\"will not\",\"n't\":\" not\",\"'re\":\" are\",\"'s\":\" is\",\"'d\":\" would\",\"'ll\":\" will\",\"'ve\":\" have\",\"'m\":\" am\"}\n",
    "NEG_CUES = {\"not\",\"never\",\"no\",\"n't\",\"cannot\"}\n",
    "STOPWORDS = {\"the\",\"and\",\"a\",\"an\",\"of\",\"in\",\"on\",\"at\",\"to\",\"is\",\"it\",\"this\",\"that\",\"i\",\"you\",\"he\",\"she\",\"they\",\"we\",\"was\",\"were\",\"be\",\"been\",\"am\",\"are\",\"but\",\"if\",\"or\",\"as\",\"with\",\"for\",\"not\",\"no\",\"so\",\"too\",\"very\"}\n",
    "\n",
    "def expand_contractions(t):\n",
    "    for p,r in CONTRACTIONS.items():\n",
    "        t = re.sub(p,r,t)\n",
    "    return t\n",
    "\n",
    "def basic_clean(t):\n",
    "    t = expand_contractions(t.lower())\n",
    "    t = HTML_PATTERN.sub(\" \",t)\n",
    "    t = URL_PATTERN.sub(\" \",t)\n",
    "    t = ALLOWED_CHARS.sub(\" \",t)\n",
    "    return re.sub(r\"\\s+\",\" \",t).strip()\n",
    "\n",
    "def clean_series(s):\n",
    "    return s.astype(str).fillna(\"\").map(basic_clean)\n",
    "\n",
    "def mark_negations(s):\n",
    "    out = []\n",
    "    for txt in s:\n",
    "        neg = False\n",
    "        buf = []\n",
    "        for tok in txt.split():\n",
    "            if tok in NEG_CUES:\n",
    "                buf.append(f\"{tok}_NEG\")\n",
    "                neg = True\n",
    "            elif neg and re.fullmatch(r\"[a-z0-9']+\",tok):\n",
    "                buf.append(f\"{tok}_NEG\")\n",
    "            else:\n",
    "                buf.append(tok)\n",
    "                if re.search(r\"[\\.\\!\\?]$\",tok):\n",
    "                    neg = False\n",
    "        out.append(\" \".join(buf))\n",
    "    return pd.Series(out,index=s.index)\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    return s.map(lambda t:\" \".join(w for w in t.split() if len(w)>1 and w not in STOPWORDS))\n",
    "\n",
    "def show_cv_table(res, cols):\n",
    "    df = pd.DataFrame(res)\n",
    "    param_cols = [f\"param_{c}\" for c in cols]\n",
    "    tbl = df[param_cols + [\"mean_test_score\", \"std_test_score\"]]\n",
    "    tbl = tbl.rename(columns=dict(zip(param_cols, cols)))\n",
    "    display(tbl.sort_values(\"mean_test_score\", ascending=False))\n",
    "\n",
    "\n",
    "def evaluate_model(y_true,y_pred,y_prob,name):\n",
    "    print(name)\n",
    "    print(\"accuracy\",accuracy_score(y_true,y_pred))\n",
    "    print(\"f1_macro\",f1_score(y_true,y_pred,average=\"macro\"))\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    plt.figure(figsize=(4,4)); plt.imshow(cm,cmap=\"Blues\"); plt.xticks([0,1]); plt.yticks([0,1])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i,str(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>cm.max()/2 else \"black\")\n",
    "    plt.show()\n",
    "    fpr,tpr,_ = roc_curve(y_true,y_prob)\n",
    "    plt.figure(); plt.plot(fpr,tpr,label=\"AUC=\"+str(auc(fpr,tpr))); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.show()\n",
    "\n",
    "def run_cv(build_fn,X,y):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    pipes,grids = build_fn()\n",
    "    best_models = {}\n",
    "    scores = {}\n",
    "    for name,pipe in pipes.items():\n",
    "        gs = GridSearchCV(pipe,grids[name],cv=cv,scoring=\"f1_macro\",n_jobs=-1,verbose=1)\n",
    "        gs.fit(X,y)\n",
    "        show_cv_table(gs.cv_results_,list(grids[name].keys()))\n",
    "        best_models[name] = gs.best_estimator_\n",
    "        if name == \"SVM\":\n",
    "            scr = cross_val_predict(best_models[name],X,y,cv=cv,method=\"decision_function\")\n",
    "            preds = (scr >= 0).astype(int)\n",
    "            probs = expit(scr)\n",
    "        else:\n",
    "            preds = cross_val_predict(best_models[name],X,y,cv=cv,method=\"predict\")\n",
    "            probs = cross_val_predict(best_models[name],X,y,cv=cv,method=\"predict_proba\")[:,1]\n",
    "        evaluate_model(y,preds,probs,name)\n",
    "        scores[name] = f1_score(y,preds,average=\"macro\")\n",
    "    best_name = max(scores,key=scores.get)\n",
    "    return best_models[best_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35708c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    }
   ],
   "source": [
    "def load_data_fp():\n",
    "    tr = pd.read_csv(\"train1.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test1.csv\").drop_duplicates()\n",
    "    tr[\"combined\"] = tr[\"reviewText\"].fillna(\"\")+\" \"+tr[\"summary\"].fillna(\"\")\n",
    "    te[\"combined\"] = te[\"reviewText\"].fillna(\"\")+\" \"+te[\"summary\"].fillna(\"\")\n",
    "    tr = tr[tr[\"combined\"].str.len()>10].reset_index(drop=True)\n",
    "    X_tr = remove_stopwords(mark_negations(clean_series(tr[\"combined\"])))\n",
    "    X_te = remove_stopwords(mark_negations(clean_series(te[\"combined\"])))\n",
    "    y = (tr[\"overall\"]>1).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return X_tr.tolist(), X_te.tolist(), y, ids\n",
    "\n",
    "def build_pipelines_fp():\n",
    "    cnt = CountVectorizer(max_features=10000,ngram_range=(1,2),stop_words=\"english\",min_df=3,max_df=0.7,binary=True)\n",
    "    tfd = TfidfVectorizer(max_features=10000,ngram_range=(1,2),stop_words=\"english\",min_df=3,max_df=0.7,sublinear_tf=True)\n",
    "    pipe_nb = Pipeline([(\"vect\",cnt),(\"clf\",MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"vect\",tfd),(\"clf\",LogisticRegression(class_weight=\"balanced\",solver=\"liblinear\",max_iter=300))])\n",
    "    pipe_svm = Pipeline([(\"vect\",TfidfVectorizer(stop_words=\"english\",lowercase=True,max_df=0.8,min_df=3,sublinear_tf=True,ngram_range=(1,2),max_features=10000)),(\"clf\",LinearSVC(class_weight=\"balanced\",max_iter=3000))])\n",
    "    grids = {\"NB\":{\"clf__alpha\":[0.1,1]},\"LR\":{\"clf__C\":[0.5,1]},\"SVM\":{\"clf__C\":[0.5,1]}}\n",
    "    return {\"NB\":pipe_nb,\"LR\":pipe_lr,\"SVM\":pipe_svm},grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr,X_te,y,ids = load_data_fp()\n",
    "    best_model = run_cv(build_pipelines_fp,X_tr,y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\":ids,\"overall\":preds}).to_csv(\"text1_output1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ff450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_mid():\n",
    "    tr = pd.read_csv(\"train1.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test1.csv\").drop_duplicates()\n",
    "    for df in (tr,te):\n",
    "        df[\"reviewText\"] = mark_negations(clean_series(df[\"reviewText\"]))\n",
    "        df[\"summary\"] = mark_negations(clean_series(df[\"summary\"]))\n",
    "        df[\"category\"] = df[\"category\"].astype(str).fillna(\"\")\n",
    "        df[\"verified\"] = df[\"verified\"].fillna(False).astype(int)\n",
    "        df[\"vote\"] = pd.to_numeric(df[\"vote\"].astype(str).str.replace(\",\",\"\"),errors=\"coerce\").fillna(0.0)\n",
    "    y = (tr[\"overall\"]>1).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return tr.drop(columns=[\"overall\"]), te, y, ids\n",
    "\n",
    "def preprocessor_mid():\n",
    "    tfidf_review = TfidfVectorizer(max_features=60000,ngram_range=(1,2),stop_words=\"english\",min_df=3,sublinear_tf=True)\n",
    "    tfidf_summary = TfidfVectorizer(max_features=15000,ngram_range=(1,1),stop_words=\"english\",min_df=2,sublinear_tf=True)\n",
    "    tfidf_category = TfidfVectorizer(analyzer=\"char\",ngram_range=(3,5),min_df=1)\n",
    "    return ColumnTransformer([\n",
    "        (\"review\",tfidf_review,\"reviewText\"),\n",
    "        (\"summary\",tfidf_summary,\"summary\"),\n",
    "        (\"category\",tfidf_category,\"category\"),\n",
    "        (\"verified\",OneHotEncoder(handle_unknown=\"ignore\"),[\"verified\"]),\n",
    "        (\"vote\",FunctionTransformer(func=np.log1p,validate=False,feature_names_out=\"one-to-one\"),[\"vote\"])\n",
    "    ],remainder=\"drop\",sparse_threshold=0.3)\n",
    "\n",
    "def build_pipelines_mid():\n",
    "    feat = preprocessor_mid()\n",
    "    pipe_nb = Pipeline([(\"feat\",feat),(\"clf\",MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"feat\",feat),(\"clf\",LogisticRegression(class_weight=\"balanced\",solver=\"liblinear\",max_iter=2000))])\n",
    "    pipe_svm = Pipeline([(\"feat\",feat),(\"clf\",LinearSVC(class_weight=\"balanced\",max_iter=3000))])\n",
    "    grids = {\"NB\":{\"clf__alpha\":[0.1,0.5,1.0]},\"LR\":{\"clf__C\":[0.5,1.0,2.0]},\"SVM\":{\"clf__C\":[0.5,1.0,2.0]}}\n",
    "    return {\"NB\":pipe_nb,\"LR\":pipe_lr,\"SVM\":pipe_svm},grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr,X_te,y,ids = load_data_mid()\n",
    "    best_model = run_cv(build_pipelines_mid,X_tr,y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\":ids,\"overall\":preds}).to_csv(\"test1_output2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d53170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_last():\n",
    "    tr = pd.read_csv(\"train1.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test1.csv\").drop_duplicates()\n",
    "    for df in (tr, te):\n",
    "        df[\"reviewText\"] = mark_negations(clean_series(df[\"reviewText\"]))\n",
    "        df[\"summary\"] = mark_negations(clean_series(df[\"summary\"]))\n",
    "        df[\"category\"] = df[\"category\"].astype(str).fillna(\"\")\n",
    "    y = (tr[\"overall\"] > 1).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    X_tr = tr[[\"reviewText\", \"summary\", \"category\"]].copy()\n",
    "    X_te = te[[\"reviewText\", \"summary\", \"category\"]].copy()\n",
    "    return X_tr, X_te, y, ids\n",
    "\n",
    "def vectoriser_last():\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"review\", TfidfVectorizer(max_features=60000, ngram_range=(1, 2), stop_words=\"english\", min_df=3, sublinear_tf=True), \"reviewText\"),\n",
    "            (\"summary\", TfidfVectorizer(max_features=15000, ngram_range=(1, 1), stop_words=\"english\", min_df=2, sublinear_tf=True), \"summary\"),\n",
    "            (\"category\", TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), min_df=1), \"category\"),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "\n",
    "def build_pipelines_last():\n",
    "    vec = vectoriser_last()\n",
    "    pipe_nb = Pipeline([(\"vec\", vec), (\"clf\", MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"vec\", vec), (\"clf\", LogisticRegression(class_weight=\"balanced\", solver=\"liblinear\", max_iter=2000))])\n",
    "    pipe_svm = Pipeline([(\"vec\", vec), (\"clf\", LinearSVC(class_weight=\"balanced\", max_iter=3000))])\n",
    "    grids = {\n",
    "        \"NB\": {\"clf__alpha\": [0.1, 0.5, 1.0]},\n",
    "        \"LR\": {\"clf__C\": [0.5, 1.0, 2.0]},\n",
    "        \"SVM\": {\"clf__C\": [0.5, 1.0, 2.0]}\n",
    "    }\n",
    "    return {\"NB\": pipe_nb, \"LR\": pipe_lr, \"SVM\": pipe_svm}, grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr, X_te, y, ids = load_data_last()\n",
    "    best_model = run_cv(build_pipelines_last, X_tr, y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\": ids, \"overall\": preds}).to_csv(\"test1_output3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8cc401b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_models[\u001b[38;5;28mmax\u001b[39m(scores, key=scores.get)]\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     X_tr, X_te, y, ids = \u001b[43mload_data_mc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     best_model = run_holdout(build_pipelines_mc, X_tr, y)\n\u001b[32m     76\u001b[39m     best_model.fit(X_tr, y)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mload_data_mc\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m te[\u001b[33m\"\u001b[39m\u001b[33mcombined\u001b[39m\u001b[33m\"\u001b[39m] = te[\u001b[33m\"\u001b[39m\u001b[33mreviewText\u001b[39m\u001b[33m\"\u001b[39m].fillna(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + te[\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m].fillna(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m tr = tr[tr[\u001b[33m\"\u001b[39m\u001b[33mcombined\u001b[39m\u001b[33m\"\u001b[39m].str.len() > \u001b[32m10\u001b[39m].reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m X_tr = \u001b[43mremove_stopwords\u001b[49m(mark_negations(clean_series(tr[\u001b[33m\"\u001b[39m\u001b[33mcombined\u001b[39m\u001b[33m\"\u001b[39m])))\n\u001b[32m     26\u001b[39m X_te = remove_stopwords(mark_negations(clean_series(te[\u001b[33m\"\u001b[39m\u001b[33mcombined\u001b[39m\u001b[33m\"\u001b[39m])))\n\u001b[32m     27\u001b[39m y = tr[\u001b[33m\"\u001b[39m\u001b[33moverall\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m).values\n",
      "\u001b[31mNameError\u001b[39m: name 'remove_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score, accuracy_score, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def load_data_mc():\n",
    "    tr = pd.read_csv(\"train5.csv\").drop_duplicates().reset_index(drop=True)\n",
    "    te = pd.read_csv(\"test5.csv\").drop_duplicates()\n",
    "    tr[\"combined\"] = tr[\"reviewText\"].fillna(\"\") + \" \" + tr[\"summary\"].fillna(\"\")\n",
    "    te[\"combined\"] = te[\"reviewText\"].fillna(\"\") + \" \" + te[\"summary\"].fillna(\"\")\n",
    "    tr = tr[tr[\"combined\"].str.len() > 10].reset_index(drop=True)\n",
    "    X_tr = remove_stopwords(mark_negations(clean_series(tr[\"combined\"])))\n",
    "    X_te = remove_stopwords(mark_negations(clean_series(te[\"combined\"])))\n",
    "    y = tr[\"overall\"].astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return X_tr.tolist(), X_te.tolist(), y, ids\n",
    "\n",
    "def vectorisers_mc():\n",
    "    cnt = CountVectorizer(max_features=10000, ngram_range=(1,2), stop_words=\"english\", min_df=3, max_df=0.7, binary=True)\n",
    "    tfd = TfidfVectorizer(max_features=10000, ngram_range=(1,2), stop_words=\"english\", min_df=3, max_df=0.7, sublinear_tf=True)\n",
    "    tfidf_svm = TfidfVectorizer(stop_words=\"english\", lowercase=True, max_df=0.8, min_df=3, sublinear_tf=True, ngram_range=(1,2), max_features=10000)\n",
    "    return cnt, tfd, tfidf_svm\n",
    "\n",
    "def build_pipelines_mc():\n",
    "    cnt, tfd, tfidf_svm = vectorisers_mc()\n",
    "    pipe_nb  = Pipeline([(\"vect\", cnt), (\"clf\", MultinomialNB())])\n",
    "    pipe_lr  = Pipeline([(\"vect\", tfd), (\"clf\", LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500))])\n",
    "    pipe_svm = Pipeline([\n",
    "        (\"tfidf\", tfidf_svm),\n",
    "        (\"select\", SelectKBest(chi2, k=6000)),\n",
    "        (\"calib\", CalibratedClassifierCV(estimator=LinearSVC(class_weight=\"balanced\", max_iter=5000), method=\"sigmoid\", cv=5, n_jobs=-1))\n",
    "    ])\n",
    "    grids = {\"NB\": {\"clf__alpha\": [0.1, 1.0]}, \"LR\": {\"clf__C\": [0.5, 1.0]}, \"SVM\": {\"calib__estimator__C\": [0.5, 1.0]}}\n",
    "    return {\"NB\": pipe_nb, \"LR\": pipe_lr, \"SVM\": pipe_svm}, grids\n",
    "\n",
    "def evaluate_model_mc(y_true, y_pred, y_score, name):\n",
    "    acc = accuracy_score(y_true, y_pred); f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    classes = np.unique(y_true); cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    plt.figure(figsize=(4,4)); plt.imshow(cm, cmap=\"Blues\"); plt.xticks(range(len(classes)), classes); plt.yticks(range(len(classes)), classes)\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            plt.text(j, i, str(cm[i,j]), ha=\"center\", va=\"center\", color=\"white\" if cm[i,j] > cm.max()/2 else \"black\")\n",
    "    plt.title(f\"{name} Confusion Matrix\"); plt.show()\n",
    "    y_bin = label_binarize(y_true, classes=classes); fpr, tpr, _ = roc_curve(y_bin.ravel(), y_score.ravel())\n",
    "    auc_val = roc_auc_score(y_true, y_score, multi_class=\"ovr\", average=\"macro\")\n",
    "    plt.figure(); plt.plot(fpr, tpr, label=f\"AUC={auc_val:.4f}\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.title(f\"{name} ROC\"); plt.show()\n",
    "    print(name, \"accuracy\", acc, \"f1_macro\", f1, \"roc_auc_macro\", auc_val); return f1\n",
    "\n",
    "def run_holdout(build_fn, X, y):\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    pipes, grids = build_fn(); best_models = {}; scores = {}\n",
    "    for name, pipe in pipes.items():\n",
    "        gs = GridSearchCV(pipe, grids[name], cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring=\"f1_macro\", n_jobs=-1, verbose=1)\n",
    "        gs.fit(X_tr, y_tr); show_cv_table(gs.cv_results_, list(grids[name].keys()))\n",
    "        best_models[name] = gs.best_estimator_\n",
    "        y_pred = best_models[name].predict(X_val); y_proba = best_models[name].predict_proba(X_val)\n",
    "        scores[name] = evaluate_model_mc(y_val, y_pred, y_proba, name)\n",
    "    return best_models[max(scores, key=scores.get)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr, X_te, y, ids = load_data_mc()\n",
    "    best_model = run_holdout(build_pipelines_mc, X_tr, y)\n",
    "    best_model.fit(X_tr, y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\": ids, \"overall\": preds}).to_csv(\"text1_output4_mc.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train = pd.read_csv(\"train5.csv\").drop_duplicates().reset_index(drop=True)\n",
    "test = pd.read_csv(\"test5.csv\").reset_index(drop=True)\n",
    "\n",
    "for df in (train, test):\n",
    "    df[\"reviewText\"] = df[\"reviewText\"].fillna(\"\").astype(str)\n",
    "    df[\"summary\"] = df[\"summary\"].fillna(\"\").astype(str)\n",
    "    df[\"combined\"] = df[\"reviewText\"] + \" \" + df[\"summary\"]\n",
    "\n",
    "train = train[train[\"combined\"].str.len() > 10].reset_index(drop=True)\n",
    "\n",
    "def preprocess_text(s):\n",
    "    return remove_stopwords(mark_negations(clean_series(s))).tolist()\n",
    "\n",
    "train_texts = preprocess_text(train[\"combined\"])\n",
    "test_texts = preprocess_text(test[\"combined\"])\n",
    "y_train = train[\"overall\"].astype(int).values\n",
    "test_ids = test[\"id\"].tolist()\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    max_features=10000, ngram_range=(1,2),\n",
    "    stop_words=\"english\", min_df=3, max_df=0.7, binary=True\n",
    ")\n",
    "X_count = count_vect.fit_transform(train_texts)\n",
    "Xc_test = count_vect.transform(test_texts)\n",
    "\n",
    "word_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\", ngram_range=(1,2),\n",
    "    stop_words=\"english\", min_df=3, max_df=0.7,\n",
    "    max_features=20000, sublinear_tf=True\n",
    ")\n",
    "char_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"char\", ngram_range=(3,5),\n",
    "    min_df=3, max_df=0.7, max_features=30000, sublinear_tf=True\n",
    ")\n",
    "X_train = sp.hstack([\n",
    "    word_tfidf.fit_transform(train_texts),\n",
    "    char_tfidf.fit_transform(train_texts)\n",
    "]).tocsr()\n",
    "X_test = sp.hstack([\n",
    "    word_tfidf.transform(test_texts),\n",
    "    char_tfidf.transform(test_texts)\n",
    "]).tocsr()\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = {}\n",
    "\n",
    "def evaluate_metrics(name, y_true, y_pred, y_score):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    roc = roc_auc_score(y_true, y_score, multi_class=\"ovr\", average=\"macro\")\n",
    "    print(f\"\\n{name}  ACC:{acc:.4f}  F1:{f1m:.4f}  AUC:{roc:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i,j],\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i,j] > cm.max()/2 else \"black\")\n",
    "    plt.xlabel(\"Pred\"); plt.ylabel(\"True\"); plt.show()\n",
    "    yb = pd.get_dummies(y_true).values\n",
    "    plt.figure()\n",
    "    for i in range(yb.shape[1]):\n",
    "        fpr, tpr, _ = roc_curve(yb[:,i], y_score[:,i])\n",
    "        plt.plot(fpr, tpr, label=f\"{name} C{i}(AUC={auc(fpr,tpr):.2f})\")\n",
    "    plt.plot([0,1], [0,1], \"k--\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.show()\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1m, \"roc_auc\": roc}\n",
    "\n",
    "nb = MultinomialNB()\n",
    "g_nb = GridSearchCV(nb, {\"alpha\":[0.1,1.0]}, cv=cv, scoring=\"f1_macro\", n_jobs=-1, verbose=1)\n",
    "g_nb.fit(X_count, y_train)\n",
    "b_nb = g_nb.best_estimator_\n",
    "p_nb = cross_val_predict(b_nb, X_count, y_train, cv=cv)\n",
    "pr_nb = cross_val_predict(b_nb, X_count, y_train, cv=cv, method=\"predict_proba\")\n",
    "results[\"NB\"] = evaluate_metrics(\"NB\", y_train, p_nb, pr_nb)\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    multi_class=\"multinomial\", solver=\"lbfgs\",\n",
    "    class_weight=\"balanced\", max_iter=500\n",
    ")\n",
    "g_lr = GridSearchCV(lr, {\"C\":[0.5,1.0]}, cv=cv, scoring=\"f1_macro\", n_jobs=-1, verbose=1)\n",
    "g_lr.fit(X_train, y_train)\n",
    "b_lr = g_lr.best_estimator_\n",
    "p_lr = cross_val_predict(b_lr, X_train, y_train, cv=cv)\n",
    "pr_lr = cross_val_predict(b_lr, X_train, y_train, cv=cv, method=\"predict_proba\")\n",
    "results[\"LR\"] = evaluate_metrics(\"LR\", y_train, p_lr, pr_lr)\n",
    "\n",
    "svm_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        stop_words=\"english\", lowercase=True,\n",
    "        max_df=0.8, min_df=3, sublinear_tf=True,\n",
    "        ngram_range=(1,2), max_features=10000\n",
    "    )),\n",
    "    (\"select\", SelectKBest(chi2, k=6000)),\n",
    "    (\"calib\", CalibratedClassifierCV(\n",
    "        estimator=LinearSVC(class_weight=\"balanced\", max_iter=5000),\n",
    "        method=\"sigmoid\", cv=5, n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "g_svm = GridSearchCV(svm_pipe, {\"calib__estimator__C\":[0.5,1.0]}, cv=cv, scoring=\"f1_macro\", n_jobs=-1, verbose=1)\n",
    "g_svm.fit(train_texts, y_train)\n",
    "b_svm = g_svm.best_estimator_\n",
    "p_svm = cross_val_predict(b_svm, train_texts, y_train, cv=cv, method=\"predict\")\n",
    "pr_svm = cross_val_predict(b_svm, train_texts, y_train, cv=cv, method=\"predict_proba\")\n",
    "results[\"SVM\"] = evaluate_metrics(\"SVM\", y_train, p_svm, pr_svm)\n",
    "\n",
    "best = max(results, key=lambda k: results[k][\"f1_macro\"])\n",
    "print(f\"Best:{best} F1={results[best]['f1_macro']:.4f}\")\n",
    "\n",
    "if best == \"NB\":\n",
    "    final = b_nb.predict(Xc_test)\n",
    "elif best == \"LR\":\n",
    "    final = b_lr.predict(X_test)\n",
    "else:\n",
    "    final = b_svm.predict(test_texts)\n",
    "\n",
    "pd.DataFrame({\"id\": test_ids, \"overall\": final}).to_csv(\"submission_multiclass_full_metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "TEXT_COLS = [\"reviewText\", \"summary\"]\n",
    "LABEL_COL = \"category\"\n",
    "N_COMPONENTS = 120\n",
    "N_INIT = 20\n",
    "RANDOM_STATE = 42\n",
    "MIN_DF = 3\n",
    "MAX_FEATURES = 65000\n",
    "NGRAM_RANGE = (1, 2)\n",
    "BASELINE_SIL = 0.050\n",
    "\n",
    "train_df = shuffle(pd.read_csv(\"train5.csv\"), random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "test_df = shuffle(pd.read_csv(\"test5.csv\"), random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "train_combined = train_df[TEXT_COLS].fillna(\"\").agg(\" \".join, axis=1)\n",
    "test_combined = test_df[TEXT_COLS].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "train_texts = remove_stopwords(mark_negations(clean_series(train_combined)))\n",
    "test_texts = remove_stopwords(mark_negations(clean_series(test_combined)))\n",
    "\n",
    "steps = [\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=False,\n",
    "        stop_words=\"english\",\n",
    "        min_df=MIN_DF,\n",
    "        max_features=MAX_FEATURES,\n",
    "        ngram_range=NGRAM_RANGE,\n",
    "        sublinear_tf=True\n",
    "    ))\n",
    "]\n",
    "\n",
    "if N_COMPONENTS:\n",
    "    steps.append((\"svd\", TruncatedSVD(n_components=N_COMPONENTS, random_state=RANDOM_STATE)))\n",
    "\n",
    "vector_pipe = Pipeline(steps)\n",
    "\n",
    "X_train = vector_pipe.fit_transform(train_texts)\n",
    "X_test = vector_pipe.transform(test_texts)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_test = le.fit_transform(test_df[LABEL_COL])\n",
    "\n",
    "n_clusters = len(le.classes_)\n",
    "print(f\"Clustering into {n_clusters} clusters.\")\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    n_init=N_INIT,\n",
    "    max_iter=300,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "pred_clusters = km.fit_predict(X_test)\n",
    "\n",
    "sil = silhouette_score(X_test, pred_clusters, metric=\"cosine\")\n",
    "ari = adjusted_rand_score(y_test, pred_clusters)\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\"Silhouette score (cosine): {sil:.4f}\")\n",
    "print(f\"Adjusted Rand Index:       {ari:.4f}\")\n",
    "print(\"Passed the baseline silhouette threshold!\" if sil >= BASELINE_SIL else \"Below the baseline silhouette threshold.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crosstab = pd.crosstab(pred_clusters, y_test, rownames=[\"Cluster\"], colnames=[\"True Label\"])\n",
    "    print(\"Cluster ↔ True-label contingency:\")\n",
    "    print(crosstab.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59d1c0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 133\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mNB\u001b[39m\u001b[33m\"\u001b[39m:pipe_nb,\u001b[33m\"\u001b[39m\u001b[33mLR\u001b[39m\u001b[33m\"\u001b[39m:pipe_lr,\u001b[33m\"\u001b[39m\u001b[33mSVM\u001b[39m\u001b[33m\"\u001b[39m:pipe_svm},grids\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     X_tr,X_te,y,ids = \u001b[43mload_data_fp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     best_model = run_cv(build_pipelines_fp,X_tr,y)\n\u001b[32m    135\u001b[39m     preds = best_model.predict(X_te)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mload_data_fp\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data_fp\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     tr = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain2.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.drop_duplicates()\n\u001b[32m    113\u001b[39m     te = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mtest2.csv\u001b[39m\u001b[33m\"\u001b[39m).drop_duplicates()\n\u001b[32m    114\u001b[39m     tr[\u001b[33m\"\u001b[39m\u001b[33mcombined\u001b[39m\u001b[33m\"\u001b[39m] = tr[\u001b[33m\"\u001b[39m\u001b[33mreviewText\u001b[39m\u001b[33m\"\u001b[39m].fillna(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)+\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m+tr[\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m].fillna(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_latest\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_latest\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_latest\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_latest\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_latest\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'train2.csv'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from scipy.special import expit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import (accuracy_score,confusion_matrix, f1_score, roc_auc_score, roc_curve, auc)\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "URL_PATTERN = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "HTML_PATTERN = re.compile(r\"<.*?>\")\n",
    "ALLOWED_CHARS = re.compile(r\"[^a-z0-9\\.\\!\\?\\'\\s]\")\n",
    "CONTRACTIONS = {\"can't\":\"cannot\",\"won't\":\"will not\",\"n't\":\" not\",\"'re\":\" are\",\"'s\":\" is\",\"'d\":\" would\",\"'ll\":\" will\",\"'ve\":\" have\",\"'m\":\" am\"}\n",
    "NEG_CUES = {\"not\",\"never\",\"no\",\"n't\",\"cannot\"}\n",
    "STOPWORDS = {\"the\",\"and\",\"a\",\"an\",\"of\",\"in\",\"on\",\"at\",\"to\",\"is\",\"it\",\"this\",\"that\",\"i\",\"you\",\"he\",\"she\",\"they\",\"we\",\"was\",\"were\",\"be\",\"been\",\"am\",\"are\",\"but\",\"if\",\"or\",\"as\",\"with\",\"for\",\"not\",\"no\",\"so\",\"too\",\"very\"}\n",
    "\n",
    "def expand_contractions(t):\n",
    "    for p,r in CONTRACTIONS.items():\n",
    "        t = re.sub(p,r,t)\n",
    "    return t\n",
    "\n",
    "def basic_clean(t):\n",
    "    t = expand_contractions(t.lower())\n",
    "    t = HTML_PATTERN.sub(\" \",t)\n",
    "    t = URL_PATTERN.sub(\" \",t)\n",
    "    t = ALLOWED_CHARS.sub(\" \",t)\n",
    "    return re.sub(r\"\\s+\",\" \",t).strip()\n",
    "\n",
    "def clean_series(s):\n",
    "    return s.astype(str).fillna(\"\").map(basic_clean)\n",
    "\n",
    "def mark_negations(s):\n",
    "    out = []\n",
    "    for txt in s:\n",
    "        neg = False\n",
    "        buf = []\n",
    "        for tok in txt.split():\n",
    "            if tok in NEG_CUES:\n",
    "                buf.append(f\"{tok}_NEG\")\n",
    "                neg = True\n",
    "            elif neg and re.fullmatch(r\"[a-z0-9']+\",tok):\n",
    "                buf.append(f\"{tok}_NEG\")\n",
    "            else:\n",
    "                buf.append(tok)\n",
    "                if re.search(r\"[\\.\\!\\?]$\",tok):\n",
    "                    neg = False\n",
    "        out.append(\" \".join(buf))\n",
    "    return pd.Series(out,index=s.index)\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    return s.map(lambda t:\" \".join(w for w in t.split() if len(w)>1 and w not in STOPWORDS))\n",
    "\n",
    "def show_cv_table(res, cols):\n",
    "    df = pd.DataFrame(res)\n",
    "    param_cols = [f\"param_{c}\" for c in cols]\n",
    "    tbl = df[param_cols + [\"mean_test_score\", \"std_test_score\"]]\n",
    "    tbl = tbl.rename(columns=dict(zip(param_cols, cols)))\n",
    "    display(tbl.sort_values(\"mean_test_score\", ascending=False))\n",
    "\n",
    "\n",
    "def evaluate_model(y_true,y_pred,y_prob,name):\n",
    "    print(name)\n",
    "    print(\"accuracy\",accuracy_score(y_true,y_pred))\n",
    "    print(\"f1_macro\",f1_score(y_true,y_pred,average=\"macro\"))\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    plt.figure(figsize=(4,4)); plt.imshow(cm,cmap=\"Blues\"); plt.xticks([0,1]); plt.yticks([0,1])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i,str(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>cm.max()/2 else \"black\")\n",
    "    plt.show()\n",
    "    fpr,tpr,_ = roc_curve(y_true,y_prob)\n",
    "    plt.figure(); plt.plot(fpr,tpr,label=\"AUC=\"+str(auc(fpr,tpr))); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.show()\n",
    "\n",
    "def run_cv(build_fn,X,y):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    pipes,grids = build_fn()\n",
    "    best_models = {}\n",
    "    scores = {}\n",
    "    for name,pipe in pipes.items():\n",
    "        gs = GridSearchCV(pipe,grids[name],cv=cv,scoring=\"f1_macro\",n_jobs=-1,verbose=1)\n",
    "        gs.fit(X,y)\n",
    "        show_cv_table(gs.cv_results_,list(grids[name].keys()))\n",
    "        best_models[name] = gs.best_estimator_\n",
    "        if name == \"SVM\":\n",
    "            scr = cross_val_predict(best_models[name],X,y,cv=cv,method=\"decision_function\")\n",
    "            preds = (scr >= 0).astype(int)\n",
    "            probs = expit(scr)\n",
    "        else:\n",
    "            preds = cross_val_predict(best_models[name],X,y,cv=cv,method=\"predict\")\n",
    "            probs = cross_val_predict(best_models[name],X,y,cv=cv,method=\"predict_proba\")[:,1]\n",
    "        evaluate_model(y,preds,probs,name)\n",
    "        scores[name] = f1_score(y,preds,average=\"macro\")\n",
    "    best_name = max(scores,key=scores.get)\n",
    "    return best_models[best_name]\n",
    "\n",
    "def load_data_fp():\n",
    "    tr = pd.read_csv(\"train2.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test2.csv\").drop_duplicates()\n",
    "    tr[\"combined\"] = tr[\"reviewText\"].fillna(\"\")+\" \"+tr[\"summary\"].fillna(\"\")\n",
    "    te[\"combined\"] = te[\"reviewText\"].fillna(\"\")+\" \"+te[\"summary\"].fillna(\"\")\n",
    "    tr = tr[tr[\"combined\"].str.len()>10].reset_index(drop=True)\n",
    "    X_tr = remove_stopwords(mark_negations(clean_series(tr[\"combined\"])))\n",
    "    X_te = remove_stopwords(mark_negations(clean_series(te[\"combined\"])))\n",
    "    y = (tr[\"overall\"]>2).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return X_tr.tolist(), X_te.tolist(), y, ids\n",
    "\n",
    "def build_pipelines_fp():\n",
    "    cnt = CountVectorizer(max_features=10000,ngram_range=(1,2),stop_words=\"english\",min_df=3,max_df=0.7,binary=True)\n",
    "    tfd = TfidfVectorizer(max_features=10000,ngram_range=(1,2),stop_words=\"english\",min_df=3,max_df=0.7,sublinear_tf=True)\n",
    "    pipe_nb = Pipeline([(\"vect\",cnt),(\"clf\",MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"vect\",tfd),(\"clf\",LogisticRegression(class_weight=\"balanced\",solver=\"liblinear\",max_iter=300))])\n",
    "    pipe_svm = Pipeline([(\"vect\",TfidfVectorizer(stop_words=\"english\",lowercase=True,max_df=0.8,min_df=3,sublinear_tf=True,ngram_range=(1,2),max_features=10000)),(\"clf\",LinearSVC(class_weight=\"balanced\",max_iter=3000))])\n",
    "    grids = {\"NB\":{\"clf__alpha\":[0.1,1]},\"LR\":{\"clf__C\":[0.5,1]},\"SVM\":{\"clf__C\":[0.5,1]}}\n",
    "    return {\"NB\":pipe_nb,\"LR\":pipe_lr,\"SVM\":pipe_svm},grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr,X_te,y,ids = load_data_fp()\n",
    "    best_model = run_cv(build_pipelines_fp,X_tr,y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\":ids,\"overall\":preds}).to_csv(\"text2_output1.csv\",index=False)\n",
    "\n",
    "def load_data_mid():\n",
    "    tr = pd.read_csv(\"train2.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test2.csv\").drop_duplicates()\n",
    "    for df in (tr,te):\n",
    "        df[\"reviewText\"] = mark_negations(clean_series(df[\"reviewText\"]))\n",
    "        df[\"summary\"] = mark_negations(clean_series(df[\"summary\"]))\n",
    "        df[\"category\"] = df[\"category\"].astype(str).fillna(\"\")\n",
    "        df[\"verified\"] = df[\"verified\"].fillna(False).astype(int)\n",
    "        df[\"vote\"] = pd.to_numeric(df[\"vote\"].astype(str).str.replace(\",\",\"\"),errors=\"coerce\").fillna(0.0)\n",
    "    y = (tr[\"overall\"]>2).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return tr.drop(columns=[\"overall\"]), te, y, ids\n",
    "\n",
    "def preprocessor_mid():\n",
    "    tfidf_review = TfidfVectorizer(max_features=60000,ngram_range=(1,2),stop_words=\"english\",min_df=3,sublinear_tf=True)\n",
    "    tfidf_summary = TfidfVectorizer(max_features=15000,ngram_range=(1,1),stop_words=\"english\",min_df=2,sublinear_tf=True)\n",
    "    tfidf_category = TfidfVectorizer(analyzer=\"char\",ngram_range=(3,5),min_df=1)\n",
    "    return ColumnTransformer([\n",
    "        (\"review\",tfidf_review,\"reviewText\"),\n",
    "        (\"summary\",tfidf_summary,\"summary\"),\n",
    "        (\"category\",tfidf_category,\"category\"),\n",
    "        (\"verified\",OneHotEncoder(handle_unknown=\"ignore\"),[\"verified\"]),\n",
    "        (\"vote\",FunctionTransformer(func=np.log1p,validate=False,feature_names_out=\"one-to-one\"),[\"vote\"])\n",
    "    ],remainder=\"drop\",sparse_threshold=0.3)\n",
    "\n",
    "def build_pipelines_mid():\n",
    "    feat = preprocessor_mid()\n",
    "    pipe_nb = Pipeline([(\"feat\",feat),(\"clf\",MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"feat\",feat),(\"clf\",LogisticRegression(class_weight=\"balanced\",solver=\"liblinear\",max_iter=2000))])\n",
    "    pipe_svm = Pipeline([(\"feat\",feat),(\"clf\",LinearSVC(class_weight=\"balanced\",max_iter=3000))])\n",
    "    grids = {\"NB\":{\"clf__alpha\":[0.1,0.5,1.0]},\"LR\":{\"clf__C\":[0.5,1.0,2.0]},\"SVM\":{\"clf__C\":[0.5,1.0,2.0]}}\n",
    "    return {\"NB\":pipe_nb,\"LR\":pipe_lr,\"SVM\":pipe_svm},grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr,X_te,y,ids = load_data_mid()\n",
    "    best_model = run_cv(build_pipelines_mid,X_tr,y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\":ids,\"overall\":preds}).to_csv(\"test2_output2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from scipy.special import expit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import (accuracy_score,confusion_matrix, f1_score, roc_auc_score, roc_curve, auc)\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "URL_PATTERN = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "HTML_PATTERN = re.compile(r\"<.*?>\")\n",
    "ALLOWED_CHARS = re.compile(r\"[^a-z0-9\\.\\!\\?\\'\\s]\")\n",
    "CONTRACTIONS = {\"can't\":\"cannot\",\"won't\":\"will not\",\"n't\":\" not\",\"'re\":\" are\",\"'s\":\" is\",\"'d\":\" would\",\"'ll\":\" will\",\"'ve\":\" have\",\"'m\":\" am\"}\n",
    "NEG_CUES = {\"not\",\"never\",\"no\",\"n't\",\"cannot\"}\n",
    "STOPWORDS = {\"the\",\"and\",\"a\",\"an\",\"of\",\"in\",\"on\",\"at\",\"to\",\"is\",\"it\",\"this\",\"that\",\"i\",\"you\",\"he\",\"she\",\"they\",\"we\",\"was\",\"were\",\"be\",\"been\",\"am\",\"are\",\"but\",\"if\",\"or\",\"as\",\"with\",\"for\",\"not\",\"no\",\"so\",\"too\",\"very\"}\n",
    "\n",
    "def expand_contractions(t):\n",
    "    for p,r in CONTRACTIONS.items():\n",
    "        t = re.sub(p,r,t)\n",
    "    return t\n",
    "\n",
    "def basic_clean(t):\n",
    "    t = expand_contractions(t.lower())\n",
    "    t = HTML_PATTERN.sub(\" \",t)\n",
    "    t = URL_PATTERN.sub(\" \",t)\n",
    "    t = ALLOWED_CHARS.sub(\" \",t)\n",
    "    return re.sub(r\"\\s+\",\" \",t).strip()\n",
    "\n",
    "def clean_series(s):\n",
    "    return s.astype(str).fillna(\"\").map(basic_clean)\n",
    "\n",
    "def mark_negations(s):\n",
    "    out = []\n",
    "    for txt in s:\n",
    "        neg = False\n",
    "        buf = []\n",
    "        for tok in txt.split():\n",
    "            if tok in NEG_CUES:\n",
    "                buf.append(f\"{tok}_NEG\")\n",
    "                neg = True\n",
    "            elif neg and re.fullmatch(r\"[a-z0-9']+\",tok):\n",
    "                buf.append(f\"{tok}_NEG\")\n",
    "            else:\n",
    "                buf.append(tok)\n",
    "                if re.search(r\"[\\.\\!\\?]$\",tok):\n",
    "                    neg = False\n",
    "        out.append(\" \".join(buf))\n",
    "    return pd.Series(out,index=s.index)\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    return s.map(lambda t:\" \".join(w for w in t.split() if len(w)>1 and w not in STOPWORDS))\n",
    "\n",
    "def show_cv_table(res, cols):\n",
    "    df = pd.DataFrame(res)\n",
    "    param_cols = [f\"param_{c}\" for c in cols]\n",
    "    tbl = df[param_cols + [\"mean_test_score\", \"std_test_score\"]]\n",
    "    tbl = tbl.rename(columns=dict(zip(param_cols, cols)))\n",
    "    display(tbl.sort_values(\"mean_test_score\", ascending=False))\n",
    "\n",
    "\n",
    "def evaluate_model(y_true,y_pred,y_prob,name):\n",
    "    print(name)\n",
    "    print(\"accuracy\",accuracy_score(y_true,y_pred))\n",
    "    print(\"f1_macro\",f1_score(y_true,y_pred,average=\"macro\"))\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    plt.figure(figsize=(4,4)); plt.imshow(cm,cmap=\"Blues\"); plt.xticks([0,1]); plt.yticks([0,1])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i,str(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>cm.max()/2 else \"black\")\n",
    "    plt.show()\n",
    "    fpr,tpr,_ = roc_curve(y_true,y_prob)\n",
    "    plt.figure(); plt.plot(fpr,tpr,label=\"AUC=\"+str(auc(fpr,tpr))); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.show()\n",
    "\n",
    "def run_cv(build_fn,X,y):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    pipes,grids = build_fn()\n",
    "    best_models = {}\n",
    "    scores = {}\n",
    "    for name,pipe in pipes.items():\n",
    "        gs = GridSearchCV(pipe,grids[name],cv=cv,scoring=\"f1_macro\",n_jobs=-1,verbose=1)\n",
    "        gs.fit(X,y)\n",
    "        show_cv_table(gs.cv_results_,list(grids[name].keys()))\n",
    "        best_models[name] = gs.best_estimator_\n",
    "        if name == \"SVM\":\n",
    "            scr = cross_val_predict(best_models[name],X,y,cv=cv,method=\"decision_function\")\n",
    "            preds = (scr >= 0).astype(int)\n",
    "            probs = expit(scr)\n",
    "        else:\n",
    "            preds = cross_val_predict(best_models[name],X,y,cv=cv,method=\"predict\")\n",
    "            probs = cross_val_predict(best_models[name],X,y,cv=cv,method=\"predict_proba\")[:,1]\n",
    "        evaluate_model(y,preds,probs,name)\n",
    "        scores[name] = f1_score(y,preds,average=\"macro\")\n",
    "    best_name = max(scores,key=scores.get)\n",
    "    return best_models[best_name]\n",
    "\n",
    "def load_data_fp():\n",
    "    tr = pd.read_csv(\"train3.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test3.csv\").drop_duplicates()\n",
    "    tr[\"combined\"] = tr[\"reviewText\"].fillna(\"\")+\" \"+tr[\"summary\"].fillna(\"\")\n",
    "    te[\"combined\"] = te[\"reviewText\"].fillna(\"\")+\" \"+te[\"summary\"].fillna(\"\")\n",
    "    tr = tr[tr[\"combined\"].str.len()>10].reset_index(drop=True)\n",
    "    X_tr = remove_stopwords(mark_negations(clean_series(tr[\"combined\"])))\n",
    "    X_te = remove_stopwords(mark_negations(clean_series(te[\"combined\"])))\n",
    "    y = (tr[\"overall\"]>3).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return X_tr.tolist(), X_te.tolist(), y, ids\n",
    "\n",
    "def build_pipelines_fp():\n",
    "    cnt = CountVectorizer(max_features=10000,ngram_range=(1,2),stop_words=\"english\",min_df=3,max_df=0.7,binary=True)\n",
    "    tfd = TfidfVectorizer(max_features=10000,ngram_range=(1,2),stop_words=\"english\",min_df=3,max_df=0.7,sublinear_tf=True)\n",
    "    pipe_nb = Pipeline([(\"vect\",cnt),(\"clf\",MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"vect\",tfd),(\"clf\",LogisticRegression(class_weight=\"balanced\",solver=\"liblinear\",max_iter=300))])\n",
    "    pipe_svm = Pipeline([(\"vect\",TfidfVectorizer(stop_words=\"english\",lowercase=True,max_df=0.8,min_df=3,sublinear_tf=True,ngram_range=(1,2),max_features=10000)),(\"clf\",LinearSVC(class_weight=\"balanced\",max_iter=3000))])\n",
    "    grids = {\"NB\":{\"clf__alpha\":[0.1,1]},\"LR\":{\"clf__C\":[0.5,1]},\"SVM\":{\"clf__C\":[0.5,1]}}\n",
    "    return {\"NB\":pipe_nb,\"LR\":pipe_lr,\"SVM\":pipe_svm},grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr,X_te,y,ids = load_data_fp()\n",
    "    best_model = run_cv(build_pipelines_fp,X_tr,y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\":ids,\"overall\":preds}).to_csv(\"text3_output1.csv\",index=False)\n",
    "\n",
    "def load_data_mid():\n",
    "    tr = pd.read_csv(\"train3.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test3.csv\").drop_duplicates()\n",
    "    for df in (tr,te):\n",
    "        df[\"reviewText\"] = mark_negations(clean_series(df[\"reviewText\"]))\n",
    "        df[\"summary\"] = mark_negations(clean_series(df[\"summary\"]))\n",
    "        df[\"category\"] = df[\"category\"].astype(str).fillna(\"\")\n",
    "        df[\"verified\"] = df[\"verified\"].fillna(False).astype(int)\n",
    "        df[\"vote\"] = pd.to_numeric(df[\"vote\"].astype(str).str.replace(\",\",\"\"),errors=\"coerce\").fillna(0.0)\n",
    "    y = (tr[\"overall\"]>3).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return tr.drop(columns=[\"overall\"]), te, y, ids\n",
    "\n",
    "def preprocessor_mid():\n",
    "    tfidf_review = TfidfVectorizer(max_features=60000,ngram_range=(1,2),stop_words=\"english\",min_df=3,sublinear_tf=True)\n",
    "    tfidf_summary = TfidfVectorizer(max_features=15000,ngram_range=(1,1),stop_words=\"english\",min_df=2,sublinear_tf=True)\n",
    "    tfidf_category = TfidfVectorizer(analyzer=\"char\",ngram_range=(3,5),min_df=1)\n",
    "    return ColumnTransformer([\n",
    "        (\"review\",tfidf_review,\"reviewText\"),\n",
    "        (\"summary\",tfidf_summary,\"summary\"),\n",
    "        (\"category\",tfidf_category,\"category\"),\n",
    "        (\"verified\",OneHotEncoder(handle_unknown=\"ignore\"),[\"verified\"]),\n",
    "        (\"vote\",FunctionTransformer(func=np.log1p,validate=False,feature_names_out=\"one-to-one\"),[\"vote\"])\n",
    "    ],remainder=\"drop\",sparse_threshold=0.3)\n",
    "\n",
    "def build_pipelines_mid():\n",
    "    feat = preprocessor_mid()\n",
    "    pipe_nb = Pipeline([(\"feat\",feat),(\"clf\",MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"feat\",feat),(\"clf\",LogisticRegression(class_weight=\"balanced\",solver=\"liblinear\",max_iter=2000))])\n",
    "    pipe_svm = Pipeline([(\"feat\",feat),(\"clf\",LinearSVC(class_weight=\"balanced\",max_iter=3000))])\n",
    "    grids = {\"NB\":{\"clf__alpha\":[0.1,0.5,1.0]},\"LR\":{\"clf__C\":[0.5,1.0,2.0]},\"SVM\":{\"clf__C\":[0.5,1.0,2.0]}}\n",
    "    return {\"NB\":pipe_nb,\"LR\":pipe_lr,\"SVM\":pipe_svm},grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr,X_te,y,ids = load_data_mid()\n",
    "    best_model = run_cv(build_pipelines_mid,X_tr,y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\":ids,\"overall\":preds}).to_csv(\"test3_output3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b42ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from scipy.special import expit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import (accuracy_score,confusion_matrix, f1_score, roc_auc_score, roc_curve, auc)\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "URL_PATTERN = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "HTML_PATTERN = re.compile(r\"<.*?>\")\n",
    "ALLOWED_CHARS = re.compile(r\"[^a-z0-9\\.\\!\\?\\'\\s]\")\n",
    "CONTRACTIONS = {\"can't\":\"cannot\",\"won't\":\"will not\",\"n't\":\" not\",\"'re\":\" are\",\"'s\":\" is\",\"'d\":\" would\",\"'ll\":\" will\",\"'ve\":\" have\",\"'m\":\" am\"}\n",
    "NEG_CUES = {\"not\",\"never\",\"no\",\"n't\",\"cannot\"}\n",
    "STOPWORDS = {\"the\",\"and\",\"a\",\"an\",\"of\",\"in\",\"on\",\"at\",\"to\",\"is\",\"it\",\"this\",\"that\",\"i\",\"you\",\"he\",\"she\",\"they\",\"we\",\"was\",\"were\",\"be\",\"been\",\"am\",\"are\",\"but\",\"if\",\"or\",\"as\",\"with\",\"for\",\"not\",\"no\",\"so\",\"too\",\"very\"}\n",
    "\n",
    "def expand_contractions(t):\n",
    "    for p,r in CONTRACTIONS.items():\n",
    "        t = re.sub(p,r,t)\n",
    "    return t\n",
    "\n",
    "def basic_clean(t):\n",
    "    t = expand_contractions(t.lower())\n",
    "    t = HTML_PATTERN.sub(\" \",t)\n",
    "    t = URL_PATTERN.sub(\" \",t)\n",
    "    t = ALLOWED_CHARS.sub(\" \",t)\n",
    "    return re.sub(r\"\\s+\",\" \",t).strip()\n",
    "\n",
    "def clean_series(s):\n",
    "    return s.astype(str).fillna(\"\").map(basic_clean)\n",
    "\n",
    "def mark_negations(s):\n",
    "    out = []\n",
    "    for txt in s:\n",
    "        neg = False\n",
    "        buf = []\n",
    "        for tok in txt.split():\n",
    "            if tok in NEG_CUES:\n",
    "                buf.append(f\"{tok}_NEG\")\n",
    "                neg = True\n",
    "            elif neg and re.fullmatch(r\"[a-z0-9']+\",tok):\n",
    "                buf.append(f\"{tok}_NEG\")\n",
    "            else:\n",
    "                buf.append(tok)\n",
    "                if re.search(r\"[\\.\\!\\?]$\",tok):\n",
    "                    neg = False\n",
    "        out.append(\" \".join(buf))\n",
    "    return pd.Series(out,index=s.index)\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    return s.map(lambda t:\" \".join(w for w in t.split() if len(w)>1 and w not in STOPWORDS))\n",
    "\n",
    "def show_cv_table(res, cols):\n",
    "    df = pd.DataFrame(res)\n",
    "    param_cols = [f\"param_{c}\" for c in cols]\n",
    "    tbl = df[param_cols + [\"mean_test_score\", \"std_test_score\"]]\n",
    "    tbl = tbl.rename(columns=dict(zip(param_cols, cols)))\n",
    "    display(tbl.sort_values(\"mean_test_score\", ascending=False))\n",
    "\n",
    "\n",
    "def evaluate_model(y_true,y_pred,y_prob,name):\n",
    "    print(name)\n",
    "    print(\"accuracy\",accuracy_score(y_true,y_pred))\n",
    "    print(\"f1_macro\",f1_score(y_true,y_pred,average=\"macro\"))\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    plt.figure(figsize=(4,4)); plt.imshow(cm,cmap=\"Blues\"); plt.xticks([0,1]); plt.yticks([0,1])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i,str(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>cm.max()/2 else \"black\")\n",
    "    plt.show()\n",
    "    fpr,tpr,_ = roc_curve(y_true,y_prob)\n",
    "    plt.figure(); plt.plot(fpr,tpr,label=\"AUC=\"+str(auc(fpr,tpr))); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.show()\n",
    "\n",
    "def run_cv(build_fn,X,y):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    pipes,grids = build_fn()\n",
    "    best_models = {}\n",
    "    scores = {}\n",
    "    for name,pipe in pipes.items():\n",
    "        gs = GridSearchCV(pipe,grids[name],cv=cv,scoring=\"f1_macro\",n_jobs=-1,verbose=1)\n",
    "        gs.fit(X,y)\n",
    "        show_cv_table(gs.cv_results_,list(grids[name].keys()))\n",
    "        best_models[name] = gs.best_estimator_\n",
    "        if name == \"SVM\":\n",
    "            scr = cross_val_predict(best_models[name],X,y,cv=cv,method=\"decision_function\")\n",
    "            preds = (scr >= 0).astype(int)\n",
    "            probs = expit(scr)\n",
    "        else:\n",
    "            preds = cross_val_predict(best_models[name],X,y,cv=cv,method=\"predict\")\n",
    "            probs = cross_val_predict(best_models[name],X,y,cv=cv,method=\"predict_proba\")[:,1]\n",
    "        evaluate_model(y,preds,probs,name)\n",
    "        scores[name] = f1_score(y,preds,average=\"macro\")\n",
    "    best_name = max(scores,key=scores.get)\n",
    "    return best_models[best_name]\n",
    "\n",
    "def load_data_fp():\n",
    "    tr = pd.read_csv(\"train4.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test4.csv\").drop_duplicates()\n",
    "    tr[\"combined\"] = tr[\"reviewText\"].fillna(\"\")+\" \"+tr[\"summary\"].fillna(\"\")\n",
    "    te[\"combined\"] = te[\"reviewText\"].fillna(\"\")+\" \"+te[\"summary\"].fillna(\"\")\n",
    "    tr = tr[tr[\"combined\"].str.len()>10].reset_index(drop=True)\n",
    "    X_tr = remove_stopwords(mark_negations(clean_series(tr[\"combined\"])))\n",
    "    X_te = remove_stopwords(mark_negations(clean_series(te[\"combined\"])))\n",
    "    y = (tr[\"overall\"]>4).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return X_tr.tolist(), X_te.tolist(), y, ids\n",
    "\n",
    "def build_pipelines_fp():\n",
    "    cnt = CountVectorizer(max_features=10000,ngram_range=(1,2),stop_words=\"english\",min_df=3,max_df=0.7,binary=True)\n",
    "    tfd = TfidfVectorizer(max_features=10000,ngram_range=(1,2),stop_words=\"english\",min_df=3,max_df=0.7,sublinear_tf=True)\n",
    "    pipe_nb = Pipeline([(\"vect\",cnt),(\"clf\",MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"vect\",tfd),(\"clf\",LogisticRegression(class_weight=\"balanced\",solver=\"liblinear\",max_iter=300))])\n",
    "    pipe_svm = Pipeline([(\"vect\",TfidfVectorizer(stop_words=\"english\",lowercase=True,max_df=0.8,min_df=3,sublinear_tf=True,ngram_range=(1,2),max_features=10000)),(\"clf\",LinearSVC(class_weight=\"balanced\",max_iter=3000))])\n",
    "    grids = {\"NB\":{\"clf__alpha\":[0.1,1]},\"LR\":{\"clf__C\":[0.5,1]},\"SVM\":{\"clf__C\":[0.5,1]}}\n",
    "    return {\"NB\":pipe_nb,\"LR\":pipe_lr,\"SVM\":pipe_svm},grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr,X_te,y,ids = load_data_fp()\n",
    "    best_model = run_cv(build_pipelines_fp,X_tr,y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\":ids,\"overall\":preds}).to_csv(\"text4_output1.csv\",index=False)\n",
    "\n",
    "def load_data_mid():\n",
    "    tr = pd.read_csv(\"train4.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test4.csv\").drop_duplicates()\n",
    "    for df in (tr,te):\n",
    "        df[\"reviewText\"] = mark_negations(clean_series(df[\"reviewText\"]))\n",
    "        df[\"summary\"] = mark_negations(clean_series(df[\"summary\"]))\n",
    "        df[\"category\"] = df[\"category\"].astype(str).fillna(\"\")\n",
    "        df[\"verified\"] = df[\"verified\"].fillna(False).astype(int)\n",
    "        df[\"vote\"] = pd.to_numeric(df[\"vote\"].astype(str).str.replace(\",\",\"\"),errors=\"coerce\").fillna(0.0)\n",
    "    y = (tr[\"overall\"]>4).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return tr.drop(columns=[\"overall\"]), te, y, ids\n",
    "\n",
    "def preprocessor_mid():\n",
    "    tfidf_review = TfidfVectorizer(max_features=60000,ngram_range=(1,2),stop_words=\"english\",min_df=3,sublinear_tf=True)\n",
    "    tfidf_summary = TfidfVectorizer(max_features=15000,ngram_range=(1,1),stop_words=\"english\",min_df=2,sublinear_tf=True)\n",
    "    tfidf_category = TfidfVectorizer(analyzer=\"char\",ngram_range=(3,5),min_df=1)\n",
    "    return ColumnTransformer([\n",
    "        (\"review\",tfidf_review,\"reviewText\"),\n",
    "        (\"summary\",tfidf_summary,\"summary\"),\n",
    "        (\"category\",tfidf_category,\"category\"),\n",
    "        (\"verified\",OneHotEncoder(handle_unknown=\"ignore\"),[\"verified\"]),\n",
    "        (\"vote\",FunctionTransformer(func=np.log1p,validate=False,feature_names_out=\"one-to-one\"),[\"vote\"])\n",
    "    ],remainder=\"drop\",sparse_threshold=0.3)\n",
    "\n",
    "def build_pipelines_mid():\n",
    "    feat = preprocessor_mid()\n",
    "    pipe_nb = Pipeline([(\"feat\",feat),(\"clf\",MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"feat\",feat),(\"clf\",LogisticRegression(class_weight=\"balanced\",solver=\"liblinear\",max_iter=2000))])\n",
    "    pipe_svm = Pipeline([(\"feat\",feat),(\"clf\",LinearSVC(class_weight=\"balanced\",max_iter=3000))])\n",
    "    grids = {\"NB\":{\"clf__alpha\":[0.1,0.5,1.0]},\"LR\":{\"clf__C\":[0.5,1.0,2.0]},\"SVM\":{\"clf__C\":[0.5,1.0,2.0]}}\n",
    "    return {\"NB\":pipe_nb,\"LR\":pipe_lr,\"SVM\":pipe_svm},grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr,X_te,y,ids = load_data_mid()\n",
    "    best_model = run_cv(build_pipelines_mid,X_tr,y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\":ids,\"overall\":preds}).to_csv(\"test4_output4.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
