{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d2b7250-ed9e-4dad-ab40-d126c1215bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from scipy.special import expit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import (accuracy_score,confusion_matrix, f1_score, roc_auc_score, roc_curve, auc)\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "URL_PATTERN = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "HTML_PATTERN = re.compile(r\"<.*?>\")\n",
    "ALLOWED_CHARS = re.compile(r\"[^a-z0-9\\.\\!\\?\\'\\s]\")\n",
    "CONTRACTIONS = {\"can't\":\"cannot\",\"won't\":\"will not\",\"n't\":\" not\",\"'re\":\" are\",\"'s\":\" is\",\"'d\":\" would\",\"'ll\":\" will\",\"'ve\":\" have\",\"'m\":\" am\"}\n",
    "NEG_CUES = {\"not\",\"never\",\"no\",\"n't\",\"cannot\"}\n",
    "STOPWORDS = {\"the\",\"and\",\"a\",\"an\",\"of\",\"in\",\"on\",\"at\",\"to\",\"is\",\"it\",\"this\",\"that\",\"i\",\"you\",\"he\",\"she\",\"they\",\"we\",\"was\",\"were\",\"be\",\"been\",\"am\",\"are\",\"but\",\"if\",\"or\",\"as\",\"with\",\"for\",\"not\",\"no\",\"so\",\"too\",\"very\"}\n",
    "\n",
    "def expand_contractions(t):\n",
    "    for p,r in CONTRACTIONS.items():\n",
    "        t = re.sub(p,r,t)\n",
    "    return t\n",
    "\n",
    "def basic_clean(t):\n",
    "    t = expand_contractions(t.lower())\n",
    "    t = HTML_PATTERN.sub(\" \",t)\n",
    "    t = URL_PATTERN.sub(\" \",t)\n",
    "    t = ALLOWED_CHARS.sub(\" \",t)\n",
    "    return re.sub(r\"\\s+\",\" \",t).strip()\n",
    "\n",
    "def clean_series(s):\n",
    "    return s.astype(str).fillna(\"\").map(basic_clean)\n",
    "\n",
    "def mark_negations(s):\n",
    "    out = []\n",
    "    for txt in s:\n",
    "        neg = False\n",
    "        buf = []\n",
    "        for tok in txt.split():\n",
    "            if tok in NEG_CUES:\n",
    "                buf.append(f\"{tok}_NEG\")\n",
    "                neg = True\n",
    "            elif neg and re.fullmatch(r\"[a-z0-9']+\",tok):\n",
    "                buf.append(f\"{tok}_NEG\")\n",
    "            else:\n",
    "                buf.append(tok)\n",
    "                if re.search(r\"[\\.\\!\\?]$\",tok):\n",
    "                    neg = False\n",
    "        out.append(\" \".join(buf))\n",
    "    return pd.Series(out,index=s.index)\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    return s.map(lambda t:\" \".join(w for w in t.split() if len(w)>1 and w not in STOPWORDS))\n",
    "\n",
    "def show_cv_table(res, cols):\n",
    "    df = pd.DataFrame(res)\n",
    "    param_cols = [f\"param_{c}\" for c in cols]\n",
    "    tbl = df[param_cols + [\"mean_test_score\", \"std_test_score\"]]\n",
    "    tbl = tbl.rename(columns=dict(zip(param_cols, cols)))\n",
    "    display(tbl.sort_values(\"mean_test_score\", ascending=False))\n",
    "\n",
    "\n",
    "def evaluate_model(y_true,y_pred,y_prob,name):\n",
    "    print(name)\n",
    "    print(\"accuracy\",accuracy_score(y_true,y_pred))\n",
    "    print(\"f1_macro\",f1_score(y_true,y_pred,average=\"macro\"))\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    plt.figure(figsize=(4,4)); plt.imshow(cm,cmap=\"Blues\"); plt.xticks([0,1]); plt.yticks([0,1])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i,str(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>cm.max()/2 else \"black\")\n",
    "    plt.show()\n",
    "    fpr,tpr,_ = roc_curve(y_true,y_prob)\n",
    "    plt.figure(); plt.plot(fpr,tpr,label=\"AUC=\"+str(auc(fpr,tpr))); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.show()\n",
    "\n",
    "def run_cv(build_fn,X,y):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    pipes,grids = build_fn()\n",
    "    best_models = {}\n",
    "    scores = {}\n",
    "    for name,pipe in pipes.items():\n",
    "        gs = GridSearchCV(pipe,grids[name],cv=cv,scoring=\"f1_macro\",n_jobs=-1,verbose=1)\n",
    "        gs.fit(X,y)\n",
    "        show_cv_table(gs.cv_results_,list(grids[name].keys()))\n",
    "        best_models[name] = gs.best_estimator_\n",
    "        if name == \"SVM\":\n",
    "            scr = cross_val_predict(best_models[name],X,y,cv=cv,method=\"decision_function\")\n",
    "            preds = (scr >= 0).astype(int)\n",
    "            probs = expit(scr)\n",
    "        else:\n",
    "            preds = cross_val_predict(best_models[name],X,y,cv=cv,method=\"predict\")\n",
    "            probs = cross_val_predict(best_models[name],X,y,cv=cv,method=\"predict_proba\")[:,1]\n",
    "        evaluate_model(y,preds,probs,name)\n",
    "        scores[name] = f1_score(y,preds,average=\"macro\")\n",
    "    best_name = max(scores,key=scores.get)\n",
    "    return best_models[best_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e35708c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mNB\u001b[39m\u001b[33m\"\u001b[39m:pipe_nb,\u001b[33m\"\u001b[39m\u001b[33mLR\u001b[39m\u001b[33m\"\u001b[39m:pipe_lr,\u001b[33m\"\u001b[39m\u001b[33mSVM\u001b[39m\u001b[33m\"\u001b[39m:pipe_svm},grids\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     X_tr,X_te,y,ids = \u001b[43mload_data_fp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     best_model = run_cv(build_pipelines_fp,X_tr,y)\n\u001b[32m     25\u001b[39m     preds = best_model.predict(X_te)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mload_data_fp\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m tr = tr[tr[\u001b[33m\"\u001b[39m\u001b[33mcombined\u001b[39m\u001b[33m\"\u001b[39m].str.len()>\u001b[32m10\u001b[39m].reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m X_tr = remove_stopwords(mark_negations(clean_series(tr[\u001b[33m\"\u001b[39m\u001b[33mcombined\u001b[39m\u001b[33m\"\u001b[39m])))\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m X_te = remove_stopwords(mark_negations(\u001b[43mclean_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mte\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcombined\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m      9\u001b[39m y = (tr[\u001b[33m\"\u001b[39m\u001b[33moverall\u001b[39m\u001b[33m\"\u001b[39m]>\u001b[32m1\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m).values\n\u001b[32m     10\u001b[39m ids = te[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m].tolist()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mclean_series\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_series\u001b[39m(s):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasic_clean\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_latest\\Lib\\site-packages\\pandas\\core\\series.py:4700\u001b[39m, in \u001b[36mSeries.map\u001b[39m\u001b[34m(self, arg, na_action)\u001b[39m\n\u001b[32m   4620\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(\n\u001b[32m   4621\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4622\u001b[39m     arg: Callable | Mapping | Series,\n\u001b[32m   4623\u001b[39m     na_action: Literal[\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   4624\u001b[39m ) -> Series:\n\u001b[32m   4625\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4626\u001b[39m \u001b[33;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[32m   4627\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4698\u001b[39m \u001b[33;03m    dtype: object\u001b[39;00m\n\u001b[32m   4699\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4700\u001b[39m     new_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4701\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor(new_values, index=\u001b[38;5;28mself\u001b[39m.index, copy=\u001b[38;5;28;01mFalse\u001b[39;00m).__finalize__(\n\u001b[32m   4702\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmap\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4703\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_latest\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_latest\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mbasic_clean\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m     40\u001b[39m t = URL_PATTERN.sub(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m,t)\n\u001b[32m     41\u001b[39m t = ALLOWED_CHARS.sub(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m,t)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m,t).strip()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def load_data_fp():\n",
    "    tr = pd.read_csv(\"train1.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test1.csv\").drop_duplicates()\n",
    "    tr[\"combined\"] = tr[\"reviewText\"].fillna(\"\")+\" \"+tr[\"summary\"].fillna(\"\")\n",
    "    te[\"combined\"] = te[\"reviewText\"].fillna(\"\")+\" \"+te[\"summary\"].fillna(\"\")\n",
    "    tr = tr[tr[\"combined\"].str.len()>10].reset_index(drop=True)\n",
    "    X_tr = remove_stopwords(mark_negations(clean_series(tr[\"combined\"])))\n",
    "    X_te = remove_stopwords(mark_negations(clean_series(te[\"combined\"])))\n",
    "    y = (tr[\"overall\"]>1).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return X_tr.tolist(), X_te.tolist(), y, ids\n",
    "\n",
    "def build_pipelines_fp():\n",
    "    cnt = CountVectorizer(max_features=10000,ngram_range=(1,2),stop_words=\"english\",min_df=3,max_df=0.7,binary=True)\n",
    "    tfd = TfidfVectorizer(max_features=10000,ngram_range=(1,2),stop_words=\"english\",min_df=3,max_df=0.7,sublinear_tf=True)\n",
    "    pipe_nb = Pipeline([(\"vect\",cnt),(\"clf\",MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"vect\",tfd),(\"clf\",LogisticRegression(class_weight=\"balanced\",solver=\"liblinear\",max_iter=300))])\n",
    "    pipe_svm = Pipeline([(\"vect\",TfidfVectorizer(stop_words=\"english\",lowercase=True,max_df=0.8,min_df=3,sublinear_tf=True,ngram_range=(1,2),max_features=10000)),(\"clf\",LinearSVC(class_weight=\"balanced\",max_iter=3000))])\n",
    "    grids = {\"NB\":{\"clf__alpha\":[0.1,1]},\"LR\":{\"clf__C\":[0.5,1]},\"SVM\":{\"clf__C\":[0.5,1]}}\n",
    "    return {\"NB\":pipe_nb,\"LR\":pipe_lr,\"SVM\":pipe_svm},grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr,X_te,y,ids = load_data_fp()\n",
    "    best_model = run_cv(build_pipelines_fp,X_tr,y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\":ids,\"overall\":preds}).to_csv(\"text1_output1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ff450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_mid():\n",
    "    tr = pd.read_csv(\"train.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test.csv\").drop_duplicates()\n",
    "    for df in (tr,te):\n",
    "        df[\"reviewText\"] = mark_negations(clean_series(df[\"reviewText\"]))\n",
    "        df[\"summary\"] = mark_negations(clean_series(df[\"summary\"]))\n",
    "        df[\"category\"] = df[\"category\"].astype(str).fillna(\"\")\n",
    "        df[\"verified\"] = df[\"verified\"].fillna(False).astype(int)\n",
    "        df[\"vote\"] = pd.to_numeric(df[\"vote\"].astype(str).str.replace(\",\",\"\"),errors=\"coerce\").fillna(0.0)\n",
    "    y = (tr[\"overall\"]>3).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return tr.drop(columns=[\"overall\"]), te, y, ids\n",
    "\n",
    "def preprocessor_mid():\n",
    "    tfidf_review = TfidfVectorizer(max_features=60000,ngram_range=(1,2),stop_words=\"english\",min_df=3,sublinear_tf=True)\n",
    "    tfidf_summary = TfidfVectorizer(max_features=15000,ngram_range=(1,1),stop_words=\"english\",min_df=2,sublinear_tf=True)\n",
    "    tfidf_category = TfidfVectorizer(analyzer=\"char\",ngram_range=(3,5),min_df=1)\n",
    "    return ColumnTransformer([\n",
    "        (\"review\",tfidf_review,\"reviewText\"),\n",
    "        (\"summary\",tfidf_summary,\"summary\"),\n",
    "        (\"category\",tfidf_category,\"category\"),\n",
    "        (\"verified\",OneHotEncoder(handle_unknown=\"ignore\"),[\"verified\"]),\n",
    "        (\"vote\",FunctionTransformer(func=np.log1p,validate=False,feature_names_out=\"one-to-one\"),[\"vote\"])\n",
    "    ],remainder=\"drop\",sparse_threshold=0.3)\n",
    "\n",
    "def build_pipelines_mid():\n",
    "    feat = preprocessor_mid()\n",
    "    pipe_nb = Pipeline([(\"feat\",feat),(\"clf\",MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"feat\",feat),(\"clf\",LogisticRegression(class_weight=\"balanced\",solver=\"liblinear\",max_iter=2000))])\n",
    "    pipe_svm = Pipeline([(\"feat\",feat),(\"clf\",LinearSVC(class_weight=\"balanced\",max_iter=3000))])\n",
    "    grids = {\"NB\":{\"clf__alpha\":[0.1,0.5,1.0]},\"LR\":{\"clf__C\":[0.5,1.0,2.0]},\"SVM\":{\"clf__C\":[0.5,1.0,2.0]}}\n",
    "    return {\"NB\":pipe_nb,\"LR\":pipe_lr,\"SVM\":pipe_svm},grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr,X_te,y,ids = load_data_mid()\n",
    "    best_model = run_cv(build_pipelines_mid,X_tr,y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\":ids,\"overall\":preds}).to_csv(\"test1_output2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d53170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_last():\n",
    "    tr = pd.read_csv(\"train1.csv\").drop_duplicates()\n",
    "    te = pd.read_csv(\"test1.csv\").drop_duplicates()\n",
    "    for df in (tr, te):\n",
    "        df[\"reviewText\"] = mark_negations(clean_series(df[\"reviewText\"]))\n",
    "        df[\"summary\"] = mark_negations(clean_series(df[\"summary\"]))\n",
    "        df[\"category\"] = df[\"category\"].astype(str).fillna(\"\")\n",
    "    y = (tr[\"overall\"] > 1).astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    X_tr = tr[[\"reviewText\", \"summary\", \"category\"]].copy()\n",
    "    X_te = te[[\"reviewText\", \"summary\", \"category\"]].copy()\n",
    "    return X_tr, X_te, y, ids\n",
    "\n",
    "def vectoriser_last():\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"review\", TfidfVectorizer(max_features=60000, ngram_range=(1, 2), stop_words=\"english\", min_df=3, sublinear_tf=True), \"reviewText\"),\n",
    "            (\"summary\", TfidfVectorizer(max_features=15000, ngram_range=(1, 1), stop_words=\"english\", min_df=2, sublinear_tf=True), \"summary\"),\n",
    "            (\"category\", TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), min_df=1), \"category\"),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "\n",
    "def build_pipelines_last():\n",
    "    vec = vectoriser_last()\n",
    "    pipe_nb = Pipeline([(\"vec\", vec), (\"clf\", MultinomialNB())])\n",
    "    pipe_lr = Pipeline([(\"vec\", vec), (\"clf\", LogisticRegression(class_weight=\"balanced\", solver=\"liblinear\", max_iter=2000))])\n",
    "    pipe_svm = Pipeline([(\"vec\", vec), (\"clf\", LinearSVC(class_weight=\"balanced\", max_iter=3000))])\n",
    "    grids = {\n",
    "        \"NB\": {\"clf__alpha\": [0.1, 0.5, 1.0]},\n",
    "        \"LR\": {\"clf__C\": [0.5, 1.0, 2.0]},\n",
    "        \"SVM\": {\"clf__C\": [0.5, 1.0, 2.0]}\n",
    "    }\n",
    "    return {\"NB\": pipe_nb, \"LR\": pipe_lr, \"SVM\": pipe_svm}, grids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr, X_te, y, ids = load_data_last()\n",
    "    best_model = run_cv(build_pipelines_last, X_tr, y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\": ids, \"overall\": preds}).to_csv(\"test1_output3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def load_data_mc():\n",
    "    tr = pd.read_csv(\"train.csv\").drop_duplicates().reset_index(drop=True)\n",
    "    te = pd.read_csv(\"test.csv\").drop_duplicates()\n",
    "    tr[\"combined\"] = tr[\"reviewText\"].fillna(\"\") + \" \" + tr[\"summary\"].fillna(\"\")\n",
    "    te[\"combined\"] = te[\"reviewText\"].fillna(\"\") + \" \" + te[\"summary\"].fillna(\"\")\n",
    "    tr = tr[tr[\"combined\"].str.len() > 10].reset_index(drop=True)\n",
    "    X_tr = remove_stopwords(mark_negations(clean_series(tr[\"combined\"])))\n",
    "    X_te = remove_stopwords(mark_negations(clean_series(te[\"combined\"])))\n",
    "    y = tr[\"overall\"].astype(int).values\n",
    "    ids = te[\"id\"].tolist()\n",
    "    return X_tr.tolist(), X_te.tolist(), y, ids\n",
    "\n",
    "def vectorisers_mc():\n",
    "    cnt = CountVectorizer(max_features=10000, ngram_range=(1,2), stop_words=\"english\", min_df=3, max_df=0.7, binary=True)\n",
    "    tfd = TfidfVectorizer(max_features=10000, ngram_range=(1,2), stop_words=\"english\", min_df=3, max_df=0.7, sublinear_tf=True)\n",
    "    tfidf_svm = TfidfVectorizer(stop_words=\"english\", lowercase=True, max_df=0.8, min_df=3, sublinear_tf=True, ngram_range=(1,2), max_features=10000)\n",
    "    return cnt, tfd, tfidf_svm\n",
    "\n",
    "def build_pipelines_mc():\n",
    "    cnt, tfd, tfidf_svm = vectorisers_mc()\n",
    "    pipe_nb  = Pipeline([(\"vect\", cnt), (\"clf\", MultinomialNB())])\n",
    "    pipe_lr  = Pipeline([(\"vect\", tfd), (\"clf\", LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500))])\n",
    "    pipe_svm = Pipeline([\n",
    "        (\"tfidf\", tfidf_svm),\n",
    "        (\"select\", SelectKBest(chi2, k=6000)),\n",
    "        (\"calib\", CalibratedClassifierCV(estimator=LinearSVC(class_weight=\"balanced\", max_iter=5000), method=\"sigmoid\", cv=5, n_jobs=-1))\n",
    "    ])\n",
    "    grids = {\"NB\": {\"clf__alpha\": [0.1, 1.0]}, \"LR\": {\"clf__C\": [0.5, 1.0]}, \"SVM\": {\"calib__estimator__C\": [0.5, 1.0]}}\n",
    "    return {\"NB\": pipe_nb, \"LR\": pipe_lr, \"SVM\": pipe_svm}, grids\n",
    "\n",
    "def evaluate_model_mc(y_true, y_pred, y_score, name):\n",
    "    acc = accuracy_score(y_true, y_pred); f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    classes = np.unique(y_true); cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    plt.figure(figsize=(4,4)); plt.imshow(cm, cmap=\"Blues\"); plt.xticks(range(len(classes)), classes); plt.yticks(range(len(classes)), classes)\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            plt.text(j, i, str(cm[i,j]), ha=\"center\", va=\"center\", color=\"white\" if cm[i,j] > cm.max()/2 else \"black\")\n",
    "    plt.title(f\"{name} Confusion Matrix\"); plt.show()\n",
    "    y_bin = label_binarize(y_true, classes=classes); fpr, tpr, _ = roc_curve(y_bin.ravel(), y_score.ravel())\n",
    "    auc_val = roc_auc_score(y_true, y_score, multi_class=\"ovr\", average=\"macro\")\n",
    "    plt.figure(); plt.plot(fpr, tpr, label=f\"AUC={auc_val:.4f}\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.title(f\"{name} ROC\"); plt.show()\n",
    "    print(name, \"accuracy\", acc, \"f1_macro\", f1, \"roc_auc_macro\", auc_val); return f1\n",
    "\n",
    "def run_holdout(build_fn, X, y):\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    pipes, grids = build_fn(); best_models = {}; scores = {}\n",
    "    for name, pipe in pipes.items():\n",
    "        gs = GridSearchCV(pipe, grids[name], cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring=\"f1_macro\", n_jobs=-1, verbose=1)\n",
    "        gs.fit(X_tr, y_tr); show_cv_table(gs.cv_results_, list(grids[name].keys()))\n",
    "        best_models[name] = gs.best_estimator_\n",
    "        y_pred = best_models[name].predict(X_val); y_proba = best_models[name].predict_proba(X_val)\n",
    "        scores[name] = evaluate_model_mc(y_val, y_pred, y_proba, name)\n",
    "    return best_models[max(scores, key=scores.get)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr, X_te, y, ids = load_data_mc()\n",
    "    best_model = run_holdout(build_pipelines_mc, X_tr, y)\n",
    "    best_model.fit(X_tr, y)\n",
    "    preds = best_model.predict(X_te)\n",
    "    pd.DataFrame({\"id\": ids, \"overall\": preds}).to_csv(\"text1_output4_mc.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_auc_score, f1_score,\n",
    "    accuracy_score, classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "def expand_contractions(text):\n",
    "    contractions = {\n",
    "        \"can't\":\"cannot\",\"won't\":\"will not\",\"n't\":\" not\",\"'re\":\" are\",\n",
    "        \"'s\":\" is\",\"'d\":\" would\",\"'ll\":\" will\",\"'ve\":\" have\",\"'m\":\" am\"\n",
    "    }\n",
    "    for k, v in contractions.items():\n",
    "        text = re.sub(k, v, text)\n",
    "    return text\n",
    "\n",
    "def light_clean(texts):\n",
    "    url_pat = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    html_pat = re.compile(r'<.*?>')\n",
    "    allowed = re.compile(r\"[^a-z0-9\\.\\!\\?\\'\\s]\")\n",
    "    out = []\n",
    "    for text in texts:\n",
    "        t = expand_contractions(text.lower())\n",
    "        t = html_pat.sub(' ', t)\n",
    "        t = url_pat.sub(' ', t)\n",
    "        t = allowed.sub(' ', t)\n",
    "        out.append(re.sub(r'\\s+', ' ', t).strip())\n",
    "    return out\n",
    "\n",
    "def mark_negations(texts):\n",
    "    neg_cues = {\"not\",\"never\",\"no\",\"n't\",\"cannot\"}\n",
    "    out = []\n",
    "    for text in texts:\n",
    "        tokens, neg, new = text.split(), False, []\n",
    "        for tok in tokens:\n",
    "            low = tok.lower()\n",
    "            if low in neg_cues:\n",
    "                new.append(low + \"_NEG\")\n",
    "                neg = True\n",
    "            elif neg and re.fullmatch(r\"[a-z0-9']+\", low):\n",
    "                new.append(low + \"_NEG\")\n",
    "            else:\n",
    "                new.append(low)\n",
    "                if re.search(r\"[\\.!\\?]$\", low):\n",
    "                    neg = False\n",
    "        out.append(\" \".join(new))\n",
    "    return out\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    sw = {\n",
    "        'the','and','a','an','of','in','on','at','to','is','it','this','that',\n",
    "        'i','you','he','she','they','we','was','were','be','been','am','are',\n",
    "        'but','if','or','as','with','for','not','no','so','too','very'\n",
    "    }\n",
    "    return [\n",
    "        \" \".join(tok for tok in txt.split() if len(tok)>1 and tok not in sw)\n",
    "        for txt in texts\n",
    "    ]\n",
    "\n",
    "def evaluate_metrics(name, y_true, y_pred, y_score):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_true, y_score, multi_class='ovr', average='macro')\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy    : {acc:.4f}\")\n",
    "    print(f\"Macro-F1     : {f1m:.4f}\")\n",
    "    print(f\"ROC-AUC (OvR): {roc_auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.xticks(range(cm.shape[1]))\n",
    "    plt.yticks(range(cm.shape[0]))\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i,j], ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i,j]>cm.max()/2 else \"black\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    y_bin = pd.get_dummies(y_true).values\n",
    "    fpr, tpr, aucs = {}, {}, {}\n",
    "    for i in range(y_bin.shape[1]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:,i], y_score[:,i])\n",
    "        aucs[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(y_bin.shape[1]):\n",
    "        plt.plot(fpr[i], tpr[i], label=f\"Class {i} (AUC={aucs[i]:.2f})\")\n",
    "    plt.plot([0,1], [0,1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{name} ROC Curves\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return {'accuracy': acc, 'f1_macro': f1m, 'roc_auc': roc_auc}\n",
    "\n",
    "train = pd.read_csv(\"train.csv\").drop_duplicates().reset_index(drop=True)\n",
    "test  = pd.read_csv(\"test.csv\")\n",
    "for df in (train, test):\n",
    "    df[\"reviewText\"] = df[\"reviewText\"].fillna(\"\").astype(str)\n",
    "    df[\"summary\"]    = df[\"summary\"].fillna(\"\").astype(str)\n",
    "    df[\"combined\"]   = df[\"reviewText\"] + \" \" + df[\"summary\"]\n",
    "train = train[train[\"combined\"].str.len()>10].reset_index(drop=True)\n",
    "\n",
    "train_texts = remove_stopwords(mark_negations(light_clean(train[\"combined\"])))\n",
    "test_texts  = remove_stopwords(mark_negations(light_clean(test[\"combined\"])))\n",
    "y_train     = train[\"overall\"].astype(int).values\n",
    "test_ids    = test[\"id\"].tolist()\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    max_features=10000, ngram_range=(1,2),\n",
    "    stop_words='english', min_df=3, max_df=0.7, binary=True\n",
    ")\n",
    "X_count = count_vect.fit_transform(train_texts)\n",
    "Xc_test = count_vect.transform(test_texts)\n",
    "\n",
    "word_tfidf = TfidfVectorizer(\n",
    "    analyzer='word', ngram_range=(1,2),\n",
    "    stop_words='english', min_df=3, max_df=0.7,\n",
    "    max_features=20000, sublinear_tf=True\n",
    ")\n",
    "char_tfidf = TfidfVectorizer(\n",
    "    analyzer='char', ngram_range=(3,5),\n",
    "    min_df=3, max_df=0.7, max_features=30000, sublinear_tf=True\n",
    ")\n",
    "X_train = sp.hstack([\n",
    "    word_tfidf.fit_transform(train_texts),\n",
    "    char_tfidf.fit_transform(train_texts)\n",
    "]).tocsr()\n",
    "X_test = sp.hstack([\n",
    "    word_tfidf.transform(test_texts),\n",
    "    char_tfidf.transform(test_texts)\n",
    "]).tocsr()\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = {}\n",
    "\n",
    "nb      = MultinomialNB()\n",
    "grid_nb = GridSearchCV(\n",
    "    nb, {'alpha': [0.1, 1.0]},\n",
    "    cv=cv, scoring='f1_macro', n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_nb.fit(X_count, y_train)\n",
    "best_nb     = grid_nb.best_estimator_\n",
    "y_pred_nb   = cross_val_predict(best_nb, X_count, y_train, cv=cv)\n",
    "y_proba_nb  = cross_val_predict(best_nb, X_count, y_train, cv=cv, method='predict_proba')\n",
    "results['NB'] = evaluate_metrics(\"Naïve Bayes\", y_train, y_pred_nb, y_proba_nb)\n",
    "\n",
    "lr      = LogisticRegression(\n",
    "    multi_class='multinomial', solver='lbfgs',\n",
    "    class_weight='balanced', max_iter=500\n",
    ")\n",
    "grid_lr = GridSearchCV(\n",
    "    lr, {'C': [0.5, 1.0]},\n",
    "    cv=cv, scoring='f1_macro', n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_lr.fit(X_train, y_train)\n",
    "best_lr    = grid_lr.best_estimator_\n",
    "y_pred_lr   = cross_val_predict(best_lr, X_train, y_train, cv=cv)\n",
    "y_proba_lr  = cross_val_predict(best_lr, X_train, y_train, cv=cv, method='predict_proba')\n",
    "results['LR'] = evaluate_metrics(\"Logistic Regression\", y_train, y_pred_lr, y_proba_lr)\n",
    "\n",
    "pipeline_svm = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        stop_words=\"english\", lowercase=True,\n",
    "        max_df=0.8, min_df=3, sublinear_tf=True,\n",
    "        ngram_range=(1,2), max_features=10000\n",
    "    )),\n",
    "    (\"select\", SelectKBest(chi2, k=6000)),\n",
    "    (\"calib\", CalibratedClassifierCV(\n",
    "        estimator=LinearSVC(class_weight=\"balanced\", max_iter=5000),\n",
    "        method=\"sigmoid\", cv=5, n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "grid_svm = GridSearchCV(\n",
    "    pipeline_svm,\n",
    "    {\"calib__estimator__C\": [0.5, 1.0]},\n",
    "    cv=cv, scoring=\"f1_macro\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_svm.fit(train_texts, y_train)\n",
    "best_svm     = grid_svm.best_estimator_\n",
    "y_pred_svm   = cross_val_predict(best_svm, train_texts, y_train, cv=cv, method='predict')\n",
    "y_proba_svm  = cross_val_predict(best_svm, train_texts, y_train, cv=cv, method='predict_proba')\n",
    "results['SVM'] = evaluate_metrics(\"Calibrated Linear SVC\", y_train, y_pred_svm, y_proba_svm)\n",
    "\n",
    "best_key = max(results, key=lambda k: results[k]['f1_macro'])\n",
    "print(f\"\\nBest model: {best_key} (F1-macro = {results[best_key]['f1_macro']:.4f})\")\n",
    "if best_key == 'NB':\n",
    "    final_preds = best_nb.predict(Xc_test)\n",
    "elif best_key == 'LR':\n",
    "    final_preds = best_lr.predict(X_test)\n",
    "else:\n",
    "    final_preds = best_svm.predict(test_texts)\n",
    "\n",
    "submission = pd.DataFrame({\"id\": test_ids, \"overall\": final_preds})\n",
    "submission.to_csv(\"submission_multiclass_full_metrics.csv\", index=False)\n",
    "print(\"Wrote submission_multiclass_full_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "TEXT_COLS = [\"reviewText\", \"summary\"]\n",
    "LABEL_COL = \"category\"\n",
    "N_COMPONENTS = 120\n",
    "N_INIT = 20\n",
    "RANDOM_STATE = 42\n",
    "MIN_DF = 3\n",
    "MAX_FEATURES = 65000\n",
    "NGRAM_RANGE = (1, 2)\n",
    "BASELINE_SIL = 0.050\n",
    "\n",
    "CONTRACTIONS = {\n",
    "    r\"\\bcan't\\b\": \"cannot\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"n't\\b\": \" not\",\n",
    "    r\"'re\\b\": \" are\",\n",
    "    r\"'s\\b\": \" is\",\n",
    "    r\"'d\\b\": \" would\",\n",
    "    r\"'ll\\b\": \" will\",\n",
    "    r\"'ve\\b\": \" have\",\n",
    "    r\"'m\\b\": \" am\",\n",
    "}\n",
    "\n",
    "def expand_contractions(text: str) -> str:\n",
    "    for pat, repl in CONTRACTIONS.items():\n",
    "        text = re.sub(pat, repl, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.Series:\n",
    "    text = (\n",
    "        df[TEXT_COLS]\n",
    "        .fillna(\"\")\n",
    "        .agg(\" \".join, axis=1)\n",
    "        .str.lower()\n",
    "        .map(expand_contractions)\n",
    "        .str.replace(r\"[^a-z0-9\\s]\", \" \", regex=True)\n",
    "        .str.replace(r\"\\s{2,}\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "    return text\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df = shuffle(train_df, random_state=RANDOM_STATE)\n",
    "test_df = shuffle(test_df, random_state=RANDOM_STATE)\n",
    "\n",
    "steps = [\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=False,\n",
    "        stop_words=\"english\",\n",
    "        min_df=MIN_DF,\n",
    "        max_features=MAX_FEATURES,\n",
    "        ngram_range=NGRAM_RANGE,\n",
    "        sublinear_tf=True,\n",
    "    ))\n",
    "]\n",
    "\n",
    "if N_COMPONENTS:\n",
    "    steps.append((\"svd\", TruncatedSVD(\n",
    "        n_components=N_COMPONENTS,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )))\n",
    "\n",
    "vector_pipe = Pipeline(steps)\n",
    "\n",
    "X_train = vector_pipe.fit_transform(preprocess(train_df))\n",
    "X_test = vector_pipe.transform(preprocess(test_df))\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_test = le.fit_transform(test_df[LABEL_COL])\n",
    "\n",
    "n_clusters = len(le.classes_)\n",
    "print(f\"Clustering into {n_clusters} clusters.\")\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    n_init=N_INIT,\n",
    "    max_iter=300,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "pred_clusters = km.fit_predict(X_test)\n",
    "\n",
    "sil = silhouette_score(X_test, pred_clusters, metric=\"cosine\")\n",
    "ari = adjusted_rand_score(y_test, pred_clusters)\n",
    "\n",
    "print(\"Results:\")\n",
    "print(\"Silhouette score (cosine):\", f\"{sil:.4f}\")\n",
    "print(\"Adjusted Rand Index:\", f\"{ari:.4f}\")\n",
    "\n",
    "if sil >= BASELINE_SIL:\n",
    "    print(\"Passed the baseline silhouette threshold!\")\n",
    "else:\n",
    "    print(\"Below the baseline silhouette threshold.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    crosstab = pd.crosstab(pred_clusters, y_test,\n",
    "                           rownames=[\"Cluster\"], colnames=[\"True Label\"])\n",
    "    print(\"Cluster ↔ True-label contingency:\")\n",
    "    print(crosstab.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
